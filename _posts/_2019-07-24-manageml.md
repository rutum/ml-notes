---
layout: page
title: Managing Machine Learning Experiments
description: This post describes how to document, reproduce and manage your ML experiments at scale
filter: [blog]
tags: machine_learning
categories: [machine learning, experiment management]
author: rutum
---

I run Machine Learning experiments for a living and I run an average of 50 experiments per stage of a project. For each experiment I write code for training models, identifying the right test cases and metrics, finding the right preprocessors - the list goes on. 

So how to manage these experiments? Here are a few criteria that I have: 

1. **Compatible with Git:** I manage all my code with Git and I want to make sure that experiment manager can keep track of how my code changes with time.
2. **Version control for data:** I want to be able to work with multiple versions of my test and training datasets. I need to know if any 2 datasets are duplicates of each other, so that I am running my tests on the same datasets. 
3. **Model Management:** When I run experiments and store models, I want to store models that are associated with an experiment, rather than a particular run. I need to have meta-data associated with the model that tells me information about how this model was created, data it was trained on etc. (This is also the experiment meta-data)
4. **Metrics:** I want to be able to store the output metrics for each experiment, and create new metrics by running the same model over different test datasets. 
5. **(Optional) Running experiments:** I want to be able to run experiments with a single command - this can be bath experiments, or a single experiment. I don't want to have to worry about containers and dockers and the logistics of it.
6. **(Optional) Experiment optimization:** I have so many variations and tests to try out. If a system to automatically try out these different variations for me that go beyond optimizing hyper-parameters of an algorithm, I would love to try such a system out. 

I went on a quest to find a solution to my problems. And while I was on my quest, I discovered some more criteria that I had previously not considered while evaluating tools. 

Here are some products that I have been looking at: 



<table>
  <tr>
    <th>Product</th>
    <th>Pricing</th>
    <!-- <th>Open Source</th> -->
  </tr>
  <tr>
    <td><a href="https://www.comet.ml/">Comet</a></td>
    <td>Paid</td>
    <!-- <td>No</td> -->
  </tr>
  <tr>
    <td><a href="https://neptune.ml/">neptune.ml</a></td>
    <td>Paid</td>
    <!-- <td>No</td> -->
  </tr>
  <tr>
    <td><a href="https://tensordash.ai/">Tensordash</a> </td>
    <td>Paid</td>
    <!-- <td></td> -->
  </tr>
  <tr>
    <td><a href="https://www.wandb.com/">Weights and Biases</a> </td>
    <td>Free for individuals and academics, Paid</td>
    <!-- <td>No</td> -->
  </tr>
  <tr>
    <td><a href="https://valohai.com/">Valohai</a></td>
    <td>Paid</td>
    <!-- <td></td> -->
  </tr>
  <tr>
    <td><a href="https://www.floydhub.com/product/train">FloydHub</a></td>
    <td>Paid</td>
    <!-- <td>No</td> -->
  </tr>
  <tr>
    <td><a href="https://verta.ai/">Verta.ai</a></td>
    <td colspan="2">Not Launched Yet</td>
  </tr>
  <tr>
    <td><a href="http://www.sirio-ml.com/">SirioML</a></td>
    <td colspan="2">Not Launched Yet</td>
  </tr>
</table>
<br>
Below is an impressive list of opensource tools in this space for running, managing and analyzing experiments. 
<table>
  <tr>
    <td><a href="https://mlflow.org/">MLFlow</a></td>
    <td>Free</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td><a href="dvc.org">DVC</a></td>
    <td>Free</td>
    <td>iterative.ai</td>
  </tr>
  <tr>
    <td><a href="https://guild.ai/">Guild.ml</a></td>
    <td>Free</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td><a href="http://mlmodelscope.org/">MLModelScope</a> </td>
    <td>Free</td>
    <td></td>
  </tr>  
  <tr>
    <td><a href="https://github.com/beringresearch/lab">Machine Learning Lab</a></td>
    <td>Free</td>
    <td>Yes</td>
  </tr> 
  
  <tr>
    <td><a href="https://modelchimp.com/">ModelChimp</a></td>
    <td>Free</td>
    <td></td>
  </tr>
  <tr>
    <td><a href="https://github.com/allegroai/trains">Trains</a></td>
    <td>Free</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td><a href="https://mitdbg.github.io/modeldb/">ModelDB</a></td>
    <td>Free</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td><a href="https://github.com/vivekratnavel/omniboard">Omniboard</a></td>
    <td>Free</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td><a href="https://github.com/datmo/datmo">Datmo</a></td>
    <td>Free</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td><a href="http://seba1511.net/randopt/">Randopt</a></td>
    <td>Free</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td><a href="https://github.com/studioml/studio">StudioML</a></td>
    <td>Free</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td><a href="https://github.com/kubeflow/kubeflow">KubeFlow</a></td>
    <td>Free</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td><a href="https://github.com/instacart/lore">Lore</a></td>
    <td>Free</td>
    <td>Yes</td>
  </tr>
    <tr>
    <td><a href="https://github.com/machinalis/featureforge">Featureforge</a></td>
    <td>Free</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td><a href="https://github.com/pachyderm/pachyderm">pachyderm</a></td>
    <td>Free</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td><a href="https://github.com/polyaxon/polyaxon">PolyAxon</a></td>
    <td>Free</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td>Runway</td>
    <td>Free</td>
    <td>paper: http://www.jsntsay.com/publications/tsay-sysml2018.pdf</td>
  </tr>
  <tr>
    <td><a href="https://github.com/IDSIA/sacred">Sacred</a></td>
    <td>Free</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td><a href="http://neuralensemble.org/sumatra/">Sumatra</a></td>
    <td>Free</td>
    <td>Yes</td>
  </tr>
</table>


















